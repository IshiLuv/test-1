{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2498abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterator, List, Optional, Protocol, Tuple\n",
    "from typing import Iterator, Optional, Sequence, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89436f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def info(self, msg: str) -> None:\n",
    "        print(f\"[INFO] {msg}\")\n",
    "\n",
    "    def warn(self, msg: str) -> None:\n",
    "        print(f\"[WARN] {msg}\")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    # paths\n",
    "    data_dir: Path = Path(\"data/competition\")\n",
    "    train_file: str = \"beauty_train.parquet\"\n",
    "    test_file: str = \"beauty_test.parquet\"\n",
    "\n",
    "    # columns\n",
    "    row_id: str = \"id\"\n",
    "    id1: str = \"id1\"\n",
    "    id2: str = \"id2\"\n",
    "    target: str = \"target\"\n",
    "\n",
    "    # features\n",
    "    cat_features: Tuple[str, ...] = (\"parentname1\", \"parentname2\", \"subjectname1\", \"subjectname2\")\n",
    "    text_features: Tuple[str, ...] = (\"title1\", \"title2\", \"description1\", \"description2\")\n",
    "\n",
    "    # drop\n",
    "    drop_cols: Tuple[str, ...] = (\"characteristics1\", \"characteristics2\")\n",
    "\n",
    "    # split\n",
    "    test_size: float = 0.2\n",
    "    seed: int = 42\n",
    "\n",
    "    # CatBoost\n",
    "    task_type: str = \"GPU\"\n",
    "    eval_metric: str = \"PRAUC\"\n",
    "    verbose: int = 200\n",
    "\n",
    "    # batching\n",
    "    batch_size: int = 10_000\n",
    "\n",
    "    # output\n",
    "    submission_name: str = \"submission.csv\"\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "log = Logger()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSource(Protocol):\n",
    "    def __iter__(self) -> Iterator[pd.DataFrame]: ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PyArrowParquetBatchIterator:\n",
    "    \"\"\"\n",
    "    Iterator: выдаёт DataFrame батчами\n",
    "    Adapter: адаптирует Parquet к pandas\n",
    "    \"\"\"\n",
    "    path: Path\n",
    "    batch_size: int = 10_000\n",
    "    columns: Optional[Sequence[str]] = None\n",
    "    drop_cols: Optional[Sequence[str]] = None\n",
    "\n",
    "    def __iter__(self) -> Iterator[pd.DataFrame]:\n",
    "        pf = pq.ParquetFile(str(self.path))\n",
    "        cols: List[str] = list(self.columns) if self.columns is not None else list(pf.schema.names)\n",
    "\n",
    "        if self.columns is None and self.drop_cols:\n",
    "            drop_set = set(self.drop_cols)\n",
    "            cols = [c for c in cols if c not in drop_set]\n",
    "\n",
    "        for batch in pf.iter_batches(batch_size=self.batch_size, columns=cols):\n",
    "            df = batch.to_pandas()\n",
    "            if self.drop_cols:\n",
    "                df = df.drop(columns=[c for c in self.drop_cols if c in df.columns], errors=\"ignore\")\n",
    "            yield df\n",
    "\n",
    "\n",
    "def load_parquet_full(path: Path, drop_cols: Sequence[str] = ()) -> pd.DataFrame:\n",
    "    pf = pq.ParquetFile(str(path))\n",
    "    cols = [c for c in pf.schema.names if c not in set(drop_cols)]\n",
    "    table = pf.read(columns=cols)\n",
    "    return table.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityStrategy(Protocol):\n",
    "    def score(self, a: str, b: str) -> float: ...\n",
    "\n",
    "\n",
    "class JaccardWordStrategy:\n",
    "    def score(self, a: str, b: str) -> float:\n",
    "        sa = set((a or \"\").lower().split())\n",
    "        sb = set((b or \"\").lower().split())\n",
    "        if not sa and not sb:\n",
    "            return 1.0\n",
    "        return len(sa & sb) / max(1, len(sa | sb))\n",
    "\n",
    "\n",
    "class TokenSetLikeStrategy:\n",
    "    def score(self, a: str, b: str) -> float:\n",
    "        sa = set((a or \"\").lower().split())\n",
    "        sb = set((b or \"\").lower().split())\n",
    "        inter = len(sa & sb)\n",
    "        union = len(sa | sb)\n",
    "        if union == 0:\n",
    "            return 1.0\n",
    "        return min(1.0, 1.2 * inter / union)\n",
    "\n",
    "\n",
    "class FeatureStrategy(Protocol):\n",
    "    def transform(self, df_raw: pd.DataFrame, cfg: Config, is_train: bool) -> pd.DataFrame: ...\n",
    "\n",
    "\n",
    "class BaselineFeatures:\n",
    "    \"\"\"Strategy: ничего не добавляем.\"\"\"\n",
    "    def transform(self, df_raw: pd.DataFrame, cfg: Config, is_train: bool) -> pd.DataFrame:\n",
    "        return df_raw.copy()\n",
    "\n",
    "\n",
    "class SimilarityAugmentFeatures:\n",
    "    \"\"\"Strategy: добавляем sim_title/sim_desc.\"\"\"\n",
    "    def __init__(self, sim: SimilarityStrategy):\n",
    "        self.sim = sim\n",
    "\n",
    "    def transform(self, df_raw: pd.DataFrame, cfg: Config, is_train: bool) -> pd.DataFrame:\n",
    "        df = df_raw.copy()\n",
    "        df[\"sim_title\"] = [\n",
    "            self.sim.score(a, b) for a, b in zip(df[\"title1\"].astype(str), df[\"title2\"].astype(str))\n",
    "        ]\n",
    "        df[\"sim_desc\"] = [\n",
    "            self.sim.score(a, b) for a, b in zip(df[\"description1\"].astype(str), df[\"description2\"].astype(str))\n",
    "        ]\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a11c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Check(Protocol):\n",
    "    def set_next(self, nxt: \"Check\") -> \"Check\": ...\n",
    "    def handle(self, df: pd.DataFrame, cfg: Config) -> pd.DataFrame: ...\n",
    "\n",
    "\n",
    "class BaseCheck:\n",
    "    def __init__(self, log: Logger):\n",
    "        self.log = log\n",
    "        self._next: Optional[Check] = None\n",
    "\n",
    "    def set_next(self, nxt: Check) -> Check:\n",
    "        self._next = nxt\n",
    "        return nxt\n",
    "\n",
    "    def handle(self, df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "        return self._next.handle(df, cfg) if self._next else df\n",
    "\n",
    "\n",
    "class SchemaCheck(BaseCheck):\n",
    "    def __init__(self, log: Logger, required: Sequence[str]):\n",
    "        super().__init__(log)\n",
    "        self.required = list(required)\n",
    "\n",
    "    def handle(self, df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "        missing = [c for c in self.required if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"SchemaCheck failed. Missing: {missing}\")\n",
    "        self.log.info(\"SchemaCheck OK\")\n",
    "        return super().handle(df, cfg)\n",
    "\n",
    "\n",
    "class NullRateCheck(BaseCheck):\n",
    "    def __init__(self, log: Logger, cols: Sequence[str], max_null_rate: float = 0.35):\n",
    "        super().__init__(log)\n",
    "        self.cols = list(cols)\n",
    "        self.max_null_rate = max_null_rate\n",
    "\n",
    "    def handle(self, df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "        for c in self.cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            rate = float(df[c].isna().mean())\n",
    "            if rate > self.max_null_rate:\n",
    "                self.log.warn(f\"NullRateCheck: {c} null_rate={rate:.3f} > {self.max_null_rate}\")\n",
    "        self.log.info(\"NullRateCheck OK (warnings possible)\")\n",
    "        return super().handle(df, cfg)\n",
    "\n",
    "\n",
    "class LeakageHeuristicCheck(BaseCheck):\n",
    "    def handle(self, df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "        if df[cfg.row_id].duplicated().any():\n",
    "            raise ValueError(\"LeakageHeuristicCheck: duplicated row_id\")\n",
    "        if (df[cfg.id1] == df[cfg.id2]).any():\n",
    "            self.log.warn(\"LeakageHeuristicCheck: found id1==id2 rows (suspicious)\")\n",
    "        self.log.info(\"LeakageHeuristicCheck OK\")\n",
    "        return super().handle(df, cfg)\n",
    "\n",
    "\n",
    "class ClassBalanceCheck(BaseCheck):\n",
    "    def __init__(self, log: Logger, min_pos_rate: float = 0.001):\n",
    "        super().__init__(log)\n",
    "        self.min_pos_rate = min_pos_rate\n",
    "\n",
    "    def handle(self, df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "        pos_rate = float(df[cfg.target].astype(int).mean())\n",
    "        if pos_rate < self.min_pos_rate:\n",
    "            self.log.warn(f\"ClassBalanceCheck: pos_rate too low = {pos_rate:.6f}\")\n",
    "        self.log.info(f\"ClassBalanceCheck: pos_rate={pos_rate:.6f}\")\n",
    "        return super().handle(df, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ada6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBackend(Protocol):\n",
    "    def fit(self, train_pool: Pool, valid_pool: Pool, cfg: Config) -> None: ...\n",
    "    def predict_proba(self, pool: Pool) -> np.ndarray: ...\n",
    "\n",
    "\n",
    "class CatBoostBackend(ModelBackend):\n",
    "    def __init__(self, log: Logger):\n",
    "        self.log = log\n",
    "        self.model: Optional[CatBoostClassifier] = None\n",
    "\n",
    "    def fit(self, train_pool: Pool, valid_pool: Pool, cfg: Config) -> None:\n",
    "        self.model = CatBoostClassifier(\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=cfg.eval_metric,\n",
    "            task_type=cfg.task_type,\n",
    "            random_seed=cfg.seed,\n",
    "            verbose=cfg.verbose,\n",
    "            iterations=5000,\n",
    "            learning_rate=0.05,\n",
    "            depth=8,\n",
    "        )\n",
    "        self.model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=200, use_best_model=True)\n",
    "\n",
    "    def predict_proba(self, pool: Pool) -> np.ndarray:\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model not fitted\")\n",
    "        return self.model.predict_proba(pool)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackendProxy(ModelBackend):\n",
    "    def __init__(self, real: ModelBackend, log: Logger):\n",
    "        self.real = real\n",
    "        self.log = log\n",
    "        self._cache: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    def fit(self, train_pool: Pool, valid_pool: Pool, cfg: Config) -> None:\n",
    "        self.log.info(\"Proxy.fit -> backend.fit\")\n",
    "        self.real.fit(train_pool, valid_pool, cfg)\n",
    "        self._cache.clear()\n",
    "\n",
    "    def predict_proba(self, pool: Pool) -> np.ndarray:\n",
    "        key = str(id(pool))\n",
    "        if key in self._cache:\n",
    "            self.log.info(\"Proxy.predict_proba cache hit\")\n",
    "            return self._cache[key]\n",
    "        self.log.info(\"Proxy.predict_proba cache miss -> backend.predict_proba\")\n",
    "        pred = self.real.predict_proba(pool)\n",
    "        self._cache[key] = pred\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, backend: ModelBackend, feature_strategy: FeatureStrategy, cfg: Config, log: Logger):\n",
    "        self.backend = backend\n",
    "        self.feature_strategy = feature_strategy\n",
    "        self.cfg = cfg\n",
    "        self.log = log\n",
    "\n",
    "    def _pool(self, X: pd.DataFrame, y: Optional[pd.Series]) -> Pool:\n",
    "        return Pool(\n",
    "            X, y,\n",
    "            cat_features=list(self.cfg.cat_features),\n",
    "            text_features=list(self.cfg.text_features),\n",
    "        )\n",
    "\n",
    "    def _split_group_min_id(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        groups = np.minimum(df[self.cfg.id1].astype(str).values, df[self.cfg.id2].astype(str).values)\n",
    "        splitter = GroupShuffleSplit(n_splits=1, test_size=self.cfg.test_size, random_state=self.cfg.seed)\n",
    "        tr_idx, va_idx = next(splitter.split(df, groups=groups))\n",
    "        return df.iloc[tr_idx].copy(), df.iloc[va_idx].copy()\n",
    "\n",
    "    def fit_validate(self, df_train_raw: pd.DataFrame) -> float:\n",
    "        required = [self.cfg.row_id, self.cfg.id1, self.cfg.id2, self.cfg.target, *self.cfg.cat_features, *self.cfg.text_features]\n",
    "        chain = SchemaCheck(self.log, required)\n",
    "        chain.set_next(NullRateCheck(self.log, self.cfg.text_features, 0.35))\\\n",
    "             .set_next(LeakageHeuristicCheck(self.log))\\\n",
    "             .set_next(ClassBalanceCheck(self.log, 0.001))\n",
    "\n",
    "        df_train_raw = chain.handle(df_train_raw, self.cfg)\n",
    "\n",
    "        # Strategy\n",
    "        df_train = self.feature_strategy.transform(df_train_raw, self.cfg, is_train=True)\n",
    "\n",
    "        # Split anti-leak\n",
    "        df_tr, df_va = self._split_group_min_id(df_train)\n",
    "\n",
    "        y_tr = df_tr[self.cfg.target].astype(int)\n",
    "        y_va = df_va[self.cfg.target].astype(int)\n",
    "\n",
    "        X_tr = df_tr.drop(columns=[self.cfg.target, self.cfg.row_id, self.cfg.id1, self.cfg.id2])\n",
    "        X_va = df_va.drop(columns=[self.cfg.target, self.cfg.row_id, self.cfg.id1, self.cfg.id2])\n",
    "\n",
    "        train_pool = self._pool(X_tr, y_tr)\n",
    "        valid_pool = self._pool(X_va, y_va)\n",
    "\n",
    "        self.log.info(f\"Fit: train={len(df_tr)} valid={len(df_va)}\")\n",
    "        self.backend.fit(train_pool, valid_pool, self.cfg)\n",
    "\n",
    "        pred = self.backend.predict_proba(valid_pool)\n",
    "        ap = float(average_precision_score(y_va.values, pred))\n",
    "        base = float(y_va.mean())\n",
    "        self.log.info(f\"Valid PR-AUC(AP)={ap:.6f} baseline≈{base:.6f}\")\n",
    "        return ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_batched_to_csv(\n",
    "    cfg: Config,\n",
    "    log: Logger,\n",
    "    backend: ModelBackend,\n",
    "    feature_strategy: FeatureStrategy,\n",
    "    test_source: BatchSource,\n",
    ") -> Path:\n",
    "    out_path = cfg.data_dir / cfg.submission_name\n",
    "    first = True\n",
    "\n",
    "    for df_raw in test_source:\n",
    "        df_feat = feature_strategy.transform(df_raw, cfg, is_train=False)\n",
    "\n",
    "        X = df_feat.drop(columns=[c for c in [cfg.target, cfg.row_id, cfg.id1, cfg.id2] if c in df_feat.columns])\n",
    "\n",
    "        pool = Pool(\n",
    "            X,\n",
    "            cat_features=list(cfg.cat_features),\n",
    "            text_features=list(cfg.text_features),\n",
    "        )\n",
    "        pred = backend.predict_proba(pool)\n",
    "\n",
    "        sub_part = pd.DataFrame({cfg.row_id: df_raw[cfg.row_id].values, \"y_pred\": pred})\n",
    "        sub_part.to_csv(out_path, mode=\"w\" if first else \"a\", index=False, header=first)\n",
    "        first = False\n",
    "\n",
    "    log.info(f\"Saved submission: {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd68128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = cfg.data_dir / cfg.train_file\n",
    "test_path = cfg.data_dir / cfg.test_file\n",
    "\n",
    "feat_strategy = SimilarityAugmentFeatures(sim=JaccardWordStrategy())\n",
    "\n",
    "backend = BackendProxy(CatBoostBackend(log), log)\n",
    "\n",
    "log.info(\"Loading train parquet (full)...\")\n",
    "df_train = load_parquet_full(train_path, drop_cols=cfg.drop_cols)\n",
    "log.info(f\"Train loaded: shape={df_train.shape}\")\n",
    "\n",
    "trainer = Trainer(backend=backend, feature_strategy=feat_strategy, cfg=cfg, log=log)\n",
    "\n",
    "ap = trainer.fit_validate(df_train)\n",
    "ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92961b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Batched test inference...\")\n",
    "test_iter = PyArrowParquetBatchIterator(\n",
    "    path=test_path,\n",
    "    batch_size=cfg.batch_size,\n",
    "    columns=None,\n",
    "    drop_cols=cfg.drop_cols,\n",
    ")\n",
    "\n",
    "out_path = predict_test_batched_to_csv(cfg, log, backend, feat_strategy, test_iter)\n",
    "out_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
