{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6ce6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Protocol, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017cf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Config loaded. DATA_DIR=data\\competition\n"
     ]
    }
   ],
   "source": [
    "class _SingletonBase:\n",
    "    _instances: Dict[type, Any] = {}\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if cls not in cls._instances:\n",
    "            cls._instances[cls] = super().__new__(cls)\n",
    "        return cls._instances[cls]\n",
    "class Logger(_SingletonBase):\n",
    "    def info(self, msg: str) -> None:\n",
    "        print(f\"[INFO] {msg}\")\n",
    "\n",
    "    def warn(self, msg: str) -> None:\n",
    "        print(f\"[WARN] {msg}\")\n",
    "@dataclass\n",
    "class Config(_SingletonBase):\n",
    "    #paths\n",
    "    DATA_DIR: Path = Path(\"data/competition\")\n",
    "    submission_name: str = \"submission.csv\"\n",
    "    #columns\n",
    "    target: str = \"target\"\n",
    "    row_id: str = \"id\"\n",
    "    id1: str = \"id1\"\n",
    "    id2: str = \"id2\"ы\n",
    "    #features\n",
    "    cat_features: Tuple[str, ...] = (\"parentname1\", \"parentname2\", \"subjectname1\", \"subjectname2\")\n",
    "    text_features: Tuple[str, ...] = (\"title1\", \"title2\", \"description1\", \"description2\")\n",
    "    drop_cols: Tuple[str, ...] = (\"characteristics1\", \"characteristics2\")\n",
    "    #split\n",
    "    test_size: float = 0.2\n",
    "    random_seed: int = 42\n",
    "    split_strategy: str = \"group_min_id\"\n",
    "    #catboost\n",
    "    task_type: str = \"GPU\"\n",
    "    verbose: int = 100\n",
    "    eval_metric: str = \"PRAUC\"\n",
    "    early_stopping_rounds: int = 100\n",
    "\n",
    "cfg = Config()\n",
    "log = Logger()\n",
    "log.info(f\"Config loaded. DATA_DIR={cfg.DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e57818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFactory(Protocol):\n",
    "    def load_train(self) -> pd.DataFrame: ...\n",
    "    def load_test(self) -> pd.DataFrame: ...\n",
    "    def build_Xy(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[pd.Series]]: ...\n",
    "    def build_submission(self, test_df: pd.DataFrame, y_pred: np.ndarray) -> pd.DataFrame: ...\n",
    "\n",
    "\n",
    "class BeautyPairsFactory:\n",
    "    def __init__(self, cfg: Config, log: Logger):\n",
    "        self.cfg = cfg\n",
    "        self.log = log\n",
    "\n",
    "    def _read_parquet(self, path: Path) -> pl.LazyFrame:\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Parquet not found: {path}\")\n",
    "        return pl.scan_parquet(path)\n",
    "\n",
    "    def load_train(self) -> pd.DataFrame:\n",
    "        path = self.cfg.DATA_DIR / \"beauty_train.parquet\"\n",
    "        lf = self._read_parquet(path)\n",
    "        n = lf.select(pl.len()).collect().item()\n",
    "        self.log.info(f\"Train rows: {n}\")\n",
    "        return lf.drop(list(self.cfg.drop_cols)).collect().to_pandas()\n",
    "\n",
    "    def load_test(self) -> pd.DataFrame:\n",
    "        path = self.cfg.DATA_DIR / \"beauty_test.parquet\"\n",
    "        lf = self._read_parquet(path)\n",
    "        n = lf.select(pl.len()).collect().item()\n",
    "        self.log.info(f\"Test rows: {n}\")\n",
    "        return lf.drop(list(self.cfg.drop_cols)).collect().to_pandas()\n",
    "\n",
    "    def build_Xy(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        drop = [self.cfg.row_id, self.cfg.id1, self.cfg.id2]\n",
    "        y = None\n",
    "        if self.cfg.target in df.columns:\n",
    "            y = df[self.cfg.target].astype(int)\n",
    "            drop = drop + [self.cfg.target]\n",
    "        X = df.drop(columns=drop)\n",
    "        return X, y\n",
    "\n",
    "    def build_submission(self, test_df: pd.DataFrame, y_pred: np.ndarray) -> pd.DataFrame:\n",
    "        return pd.DataFrame({self.cfg.row_id: test_df[self.cfg.row_id].values, \"y_pred\": y_pred})\n",
    "\n",
    "\n",
    "data_factory = BeautyPairsFactory(cfg, log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCreator(Protocol):\n",
    "    def factory_method(self) -> CatBoostClassifier: ...\n",
    "\n",
    "\n",
    "class CatBoostPRAUCCreator:\n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def factory_method(self) -> CatBoostClassifier:\n",
    "        return CatBoostClassifier(\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=self.cfg.eval_metric,\n",
    "            task_type=self.cfg.task_type,\n",
    "            verbose=self.cfg.verbose,\n",
    "            random_seed=self.cfg.random_seed,\n",
    "            iterations=5000,\n",
    "            learning_rate=0.05,\n",
    "            depth=8,\n",
    "        )\n",
    "\n",
    "\n",
    "model_creator = CatBoostPRAUCCreator(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da4f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitStrategy(Protocol):\n",
    "    def split(self, df: pd.DataFrame, cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame]: ...\n",
    "\n",
    "\n",
    "class RandomStratifiedSplit:\n",
    "    def split(self, df: pd.DataFrame, cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        idx = np.arange(len(df))\n",
    "        y = df[cfg.target].astype(int).values\n",
    "        tr, va = train_test_split(\n",
    "            idx, test_size=cfg.test_size, random_state=cfg.random_seed, stratify=y\n",
    "        )\n",
    "        return df.iloc[tr].copy(), df.iloc[va].copy()\n",
    "\n",
    "\n",
    "class GroupSplitById1:\n",
    "    def split(self, df: pd.DataFrame, cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        splitter = GroupShuffleSplit(n_splits=1, test_size=cfg.test_size, random_state=cfg.random_seed)\n",
    "        groups = df[cfg.id1].values\n",
    "        tr, va = next(splitter.split(df, groups=groups))\n",
    "        return df.iloc[tr].copy(), df.iloc[va].copy()\n",
    "\n",
    "\n",
    "class GroupSplitByMinId:\n",
    "    def split(self, df: pd.DataFrame, cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        groups = np.minimum(df[cfg.id1].astype(str).values, df[cfg.id2].astype(str).values)\n",
    "        splitter = GroupShuffleSplit(n_splits=1, test_size=cfg.test_size, random_state=cfg.random_seed)\n",
    "        tr, va = next(splitter.split(df, groups=groups))\n",
    "        return df.iloc[tr].copy(), df.iloc[va].copy()\n",
    "\n",
    "\n",
    "def make_split_strategy(cfg: Config) -> SplitStrategy:\n",
    "    if cfg.split_strategy == \"random\":\n",
    "        return RandomStratifiedSplit()\n",
    "    if cfg.split_strategy == \"group_id1\":\n",
    "        return GroupSplitById1()\n",
    "    if cfg.split_strategy == \"group_min_id\":\n",
    "        return GroupSplitByMinId()\n",
    "    raise ValueError(\"split_strategy must be: random | group_id1 | group_min_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3efd40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, cfg: Config, log: Logger, data_factory: DataFactory, model: CatBoostClassifier):\n",
    "        self.cfg = cfg\n",
    "        self.log = log\n",
    "        self.data_factory = data_factory\n",
    "        self.model = model\n",
    "\n",
    "    def _pool(self, X: pd.DataFrame, y: Optional[pd.Series]) -> Pool:\n",
    "        return Pool(\n",
    "            X, y,\n",
    "            cat_features=list(self.cfg.cat_features),\n",
    "            text_features=list(self.cfg.text_features),\n",
    "        )\n",
    "\n",
    "    def fit_validate(self, train_df: pd.DataFrame) -> float:\n",
    "        splitter = make_split_strategy(self.cfg)\n",
    "        df_tr, df_va = splitter.split(train_df, self.cfg)\n",
    "\n",
    "        X_tr, y_tr = self.data_factory.build_Xy(df_tr)\n",
    "        X_va, y_va = self.data_factory.build_Xy(df_va)\n",
    "\n",
    "        train_pool = self._pool(X_tr, y_tr)\n",
    "        valid_pool = self._pool(X_va, y_va)\n",
    "\n",
    "        self.log.info(f\"Fit: train={len(df_tr)} valid={len(df_va)} split={self.cfg.split_strategy}\")\n",
    "        self.model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=self.cfg.early_stopping_rounds)\n",
    "\n",
    "        va_pred = self.model.predict_proba(valid_pool)[:, 1]\n",
    "        ap = float(average_precision_score(y_va.values, va_pred))\n",
    "        base = float(y_va.mean())\n",
    "        self.log.info(f\"Valid PR-AUC(AP)={ap:.6f} baseline≈{base:.6f}\")\n",
    "        return ap\n",
    "\n",
    "    def fit_full(self, train_df: pd.DataFrame) -> None:\n",
    "        X, y = self.data_factory.build_Xy(train_df)\n",
    "        pool = self._pool(X, y)\n",
    "        self.log.info(f\"Fit full train: n={len(train_df)}\")\n",
    "        self.model.fit(pool)\n",
    "\n",
    "    def predict_test(self, test_df: pd.DataFrame) -> np.ndarray:\n",
    "        X_test, _ = self.data_factory.build_Xy(test_df)\n",
    "        pool = self._pool(X_test, None)\n",
    "        return self.model.predict_proba(pool)[:, 1]\n",
    "\n",
    "\n",
    "class ExperimentBuilder:\n",
    "    def __init__(self, cfg: Config, log: Logger):\n",
    "        self.cfg = cfg\n",
    "        self.log = log\n",
    "        self._data_factory: Optional[DataFactory] = None\n",
    "        self._model_creator: Optional[ModelCreator] = None\n",
    "\n",
    "    def with_data_factory(self, factory: DataFactory) -> \"ExperimentBuilder\":\n",
    "        self._data_factory = factory\n",
    "        return self\n",
    "\n",
    "    def with_model_creator(self, creator: ModelCreator) -> \"ExperimentBuilder\":\n",
    "        self._model_creator = creator\n",
    "        return self\n",
    "\n",
    "    def build(self) -> Trainer:\n",
    "        if self._data_factory is None or self._model_creator is None:\n",
    "            raise RuntimeError(\"Need data_factory and model_creator\")\n",
    "        model = self._model_creator.factory_method()\n",
    "        return Trainer(self.cfg, self.log, self._data_factory, model)\n",
    "\n",
    "\n",
    "trainer = (\n",
    "    ExperimentBuilder(cfg, log)\n",
    "    .with_data_factory(data_factory)\n",
    "    .with_model_creator(model_creator)\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad27d4c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Parquet not found: data\\competition\\beauty_train.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_df = \u001b[43mdata_factory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m test_df = data_factory.load_test()\n\u001b[32m      4\u001b[39m ap = trainer.fit_validate(train_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mBeautyPairsFactory.load_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_train\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> pd.DataFrame:\n\u001b[32m     19\u001b[39m     path = \u001b[38;5;28mself\u001b[39m.cfg.DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mbeauty_train.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     lf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     n = lf.select(pl.len()).collect().item()\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mBeautyPairsFactory._read_parquet\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_parquet\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: Path) -> pl.LazyFrame:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParquet not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pl.scan_parquet(path)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Parquet not found: data\\competition\\beauty_train.parquet"
     ]
    }
   ],
   "source": [
    "train_df = data_factory.load_train()\n",
    "test_df = data_factory.load_test()\n",
    "\n",
    "ap = trainer.fit_validate(train_df)\n",
    "\n",
    "trainer.fit_full(train_df)\n",
    "test_pred = trainer.predict_test(test_df)\n",
    "\n",
    "sub = data_factory.build_submission(test_df, test_pred)\n",
    "out_path = cfg.DATA_DIR / cfg.submission_name\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "log.info(f\"Saved submission: {out_path} | rows={len(sub)} | sample:\\n{sub.head()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aa84f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
