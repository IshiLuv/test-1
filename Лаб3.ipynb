{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6ce6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Protocol, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e017cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SingletonBase:\n",
    "    _instances: Dict[type, Any] = {}\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if cls not in cls._instances:\n",
    "            cls._instances[cls] = super().__new__(cls)\n",
    "        return cls._instances[cls]\n",
    "\n",
    "\n",
    "class Logger(_SingletonBase):\n",
    "    def info(self, msg: str) -> None:\n",
    "        print(f\"[INFO] {msg}\")\n",
    "\n",
    "    def warn(self, msg: str) -> None:\n",
    "        print(f\"[WARN] {msg}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config(_SingletonBase):\n",
    "    DATA_DIR: Path = Path(\"data/competition\")\n",
    "    submission_name: str = \"submission.csv\"\n",
    "\n",
    "    target_col: str = \"target\"\n",
    "    row_id_col: str = \"id\"\n",
    "    left_id_col: str = \"id1\"\n",
    "    right_id_col: str = \"id2\"\n",
    "\n",
    "    cat_features: Tuple[str, ...] = (\"parentname1\", \"parentname2\", \"subjectname1\", \"subjectname2\")\n",
    "    text_features: Tuple[str, ...] = (\"title1\", \"title2\", \"description1\", \"description2\")\n",
    "\n",
    "    drop_cols: Tuple[str, ...] = (\"characteristics1\", \"characteristics2\")\n",
    "    test_size: float = 0.2\n",
    "    random_seed: int = 42\n",
    "\n",
    "    split_strategy: str = \"group_min_id\"\n",
    "\n",
    "    # CatBoost\n",
    "    task_type: str = \"GPU\"\n",
    "    verbose: int = 100\n",
    "    eval_metric: str = \"PRAUC\"\n",
    "    early_stopping_rounds: int = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e57818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFactory(Protocol):\n",
    "    def load_train(self) -> pd.DataFrame: ...\n",
    "    def load_test(self) -> pd.DataFrame: ...\n",
    "    def build_Xy(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[pd.Series]]: ...\n",
    "    def build_submission(self, test_df: pd.DataFrame, y_pred: np.ndarray) -> pd.DataFrame: ...\n",
    "\n",
    "\n",
    "class BeautyPairsFactory:\n",
    "    \"\"\"\n",
    "    Семейство для конкретного датасета beauty_*:\n",
    "    - читаем parquet через polars scan (как у тебя)\n",
    "    - дропаем characteristics* (бейзлайн)\n",
    "    - делаем X, y\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Config, log: Logger):\n",
    "        self.cfg = cfg\n",
    "        self.log = log\n",
    "\n",
    "    def _read_parquet(self, path: Path) -> pl.LazyFrame:\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Parquet not found: {path}\")\n",
    "        return pl.scan_parquet(path)\n",
    "\n",
    "    def load_train(self) -> pd.DataFrame:\n",
    "        path = self.cfg.DATA_DIR / \"beauty_train.parquet\"\n",
    "        lf = self._read_parquet(path)\n",
    "        n = lf.select(pl.len()).collect().item()\n",
    "        self.log.info(f\"Train rows: {n}\")\n",
    "        df = lf.drop(list(self.cfg.drop_cols)).collect().to_pandas()\n",
    "        return df\n",
    "\n",
    "    def load_test(self) -> pd.DataFrame:\n",
    "        path = self.cfg.DATA_DIR / \"beauty_test.parquet\"\n",
    "        lf = self._read_parquet(path)\n",
    "        n = lf.select(pl.len()).collect().item()\n",
    "        self.log.info(f\"Test rows: {n}\")\n",
    "        df = lf.drop(list(self.cfg.drop_cols)).collect().to_pandas()\n",
    "        return df\n",
    "\n",
    "    def build_Xy(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        drop = [self.cfg.row_id_col, self.cfg.left_id_col, self.cfg.right_id_col]\n",
    "        y = None\n",
    "        if self.cfg.target_col in df.columns:\n",
    "            y = df[self.cfg.target_col].astype(int)\n",
    "            drop = drop + [self.cfg.target_col]\n",
    "\n",
    "        X = df.drop(columns=drop)\n",
    "        return X, y\n",
    "\n",
    "    def build_submission(self, test_df: pd.DataFrame, y_pred: np.ndarray) -> pd.DataFrame:\n",
    "        sub = pd.DataFrame({self.cfg.row_id_col: test_df[self.cfg.row_id_col].values, \"y_pred\": y_pred})\n",
    "        return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCreator(Protocol):\n",
    "    def factory_method(self) -> CatBoostClassifier: ...\n",
    "\n",
    "\n",
    "class CatBoostPRAUCCreator:\n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def factory_method(self) -> CatBoostClassifier:\n",
    "        return CatBoostClassifier(\n",
    "            verbose=self.cfg.verbose,\n",
    "            eval_metric=self.cfg.eval_metric,\n",
    "            task_type=self.cfg.task_type,\n",
    "            random_seed=self.cfg.random_seed,\n",
    "            loss_function=\"Logloss\",\n",
    "            iterations=5000,\n",
    "            learning_rate=0.05,\n",
    "            depth=8,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da4f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitStrategy(Protocol):\n",
    "    def split(self, df: pd.DataFrame, cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame]: ...\n",
    "\n",
    "\n",
    "class RandomStratifiedSplit:\n",
    "    def split(self, df: pd.DataFrame, cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        X_idx = np.arange(len(df))\n",
    "        y = df[cfg.target_col].astype(int).values\n",
    "        tr, va = train_test_split(\n",
    "            X_idx,\n",
    "            test_size=cfg.test_size,\n",
    "            random_state=cfg.random_seed,\n",
    "            stratify=y,\n",
    "        )\n",
    "        return df.iloc[tr].copy(), df.iloc[va].copy()\n",
    "\n",
    "\n",
    "class GroupSplitById1:\n",
    "    def split(self, df: pd.DataFrame, cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        splitter = GroupShuffleSplit(n_splits=1, test_size=cfg.test_size, random_state=cfg.random_seed)\n",
    "        groups = df[cfg.left_id_col].values\n",
    "        tr, va = next(splitter.split(df, groups=groups))\n",
    "        return df.iloc[tr].copy(), df.iloc[va].copy()\n",
    "\n",
    "\n",
    "class GroupSplitByMinId:\n",
    "    def split(self, df: pd.DataFrame, cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        g = np.minimum(df[cfg.left_id_col].astype(str).values, df[cfg.right_id_col].astype(str).values)\n",
    "        splitter = GroupShuffleSplit(n_splits=1, test_size=cfg.test_size, random_state=cfg.random_seed)\n",
    "        tr, va = next(splitter.split(df, groups=g))\n",
    "        return df.iloc[tr].copy(), df.iloc[va].copy()\n",
    "\n",
    "\n",
    "def make_split_strategy(cfg: Config) -> SplitStrategy:\n",
    "    if cfg.split_strategy == \"random\":\n",
    "        return RandomStratifiedSplit()\n",
    "    if cfg.split_strategy == \"group_id1\":\n",
    "        return GroupSplitById1()\n",
    "    if cfg.split_strategy == \"group_min_id\":\n",
    "        return GroupSplitByMinId()\n",
    "    raise ValueError(\"split_strategy must be: random | group_id1 | group_min_id\")\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, cfg: Config, log: Logger, data_factory: DataFactory, model: CatBoostClassifier):\n",
    "        self.cfg = cfg\n",
    "        self.log = log\n",
    "        self.data_factory = data_factory\n",
    "        self.model = model\n",
    "\n",
    "    def _make_pool(self, X: pd.DataFrame, y: Optional[pd.Series]) -> Pool:\n",
    "        return Pool(\n",
    "            X,\n",
    "            y,\n",
    "            cat_features=list(self.cfg.cat_features),\n",
    "            text_features=list(self.cfg.text_features),\n",
    "        )\n",
    "\n",
    "    def fit_validate(self, train_df: pd.DataFrame) -> float:\n",
    "        splitter = make_split_strategy(self.cfg)\n",
    "        df_tr, df_va = splitter.split(train_df, self.cfg)\n",
    "\n",
    "        X_tr, y_tr = self.data_factory.build_Xy(df_tr)\n",
    "        X_va, y_va = self.data_factory.build_Xy(df_va)\n",
    "\n",
    "        train_pool = self._make_pool(X_tr, y_tr)\n",
    "        valid_pool = self._make_pool(X_va, y_va)\n",
    "\n",
    "        self.log.info(f\"Fitting: train={len(df_tr)} valid={len(df_va)} split={self.cfg.split_strategy}\")\n",
    "        self.model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=self.cfg.early_stopping_rounds)\n",
    "\n",
    "        va_pred = self.model.predict_proba(valid_pool)[:, 1]\n",
    "        ap = float(average_precision_score(y_va.values, va_pred))\n",
    "        baseline = float(y_va.mean())\n",
    "        self.log.info(f\"Valid PR-AUC(AP)={ap:.6f} baseline≈{baseline:.6f}\")\n",
    "        return ap\n",
    "\n",
    "    def fit_full(self, train_df: pd.DataFrame) -> None:\n",
    "        X, y = self.data_factory.build_Xy(train_df)\n",
    "        pool = self._make_pool(X, y)\n",
    "        self.log.info(f\"Fitting on full train: n={len(train_df)}\")\n",
    "        self.model.fit(pool)\n",
    "\n",
    "    def predict_test(self, test_df: pd.DataFrame) -> np.ndarray:\n",
    "        X_test, _ = self.data_factory.build_Xy(test_df)\n",
    "        pool = self._make_pool(X_test, None)\n",
    "        pred = self.model.predict_proba(pool)[:, 1]\n",
    "        return pred\n",
    "\n",
    "\n",
    "class ExperimentBuilder:\n",
    "    def __init__(self, cfg: Config, log: Logger):\n",
    "        self.cfg = cfg\n",
    "        self.log = log\n",
    "        self._data_factory: Optional[DataFactory] = None\n",
    "        self._model_creator: Optional[ModelCreator] = None\n",
    "\n",
    "    def with_data_factory(self, factory: DataFactory) -> \"ExperimentBuilder\":\n",
    "        self._data_factory = factory\n",
    "        return self\n",
    "\n",
    "    def with_model_creator(self, creator: ModelCreator) -> \"ExperimentBuilder\":\n",
    "        self._model_creator = creator\n",
    "        return self\n",
    "\n",
    "    def build(self) -> Trainer:\n",
    "        if self._data_factory is None or self._model_creator is None:\n",
    "            raise RuntimeError(\"ExperimentBuilder requires data_factory and model_creator\")\n",
    "        model = self._model_creator.factory_method()\n",
    "        return Trainer(self.cfg, self.log, self._data_factory, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd40f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Parquet not found: data\\competition\\beauty_train.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     30\u001b[39m     log.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved submission: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (rows=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sub)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m trainer = (\n\u001b[32m      9\u001b[39m     ExperimentBuilder(cfg, log)\n\u001b[32m     10\u001b[39m     .with_data_factory(data_factory)      \u001b[38;5;66;03m# Abstract Factory\u001b[39;00m\n\u001b[32m     11\u001b[39m     .with_model_creator(model_creator)    \u001b[38;5;66;03m# Factory Method\u001b[39;00m\n\u001b[32m     12\u001b[39m     .build()                              \u001b[38;5;66;03m# Builder\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m train_df = \u001b[43mdata_factory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m test_df = data_factory.load_test()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mBeautyPairsFactory.load_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_train\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> pd.DataFrame:\n\u001b[32m     26\u001b[39m     path = \u001b[38;5;28mself\u001b[39m.cfg.DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mbeauty_train.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     lf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     n = lf.select(pl.len()).collect().item()\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mBeautyPairsFactory._read_parquet\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_parquet\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: Path) -> pl.LazyFrame:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParquet not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pl.scan_parquet(path)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Parquet not found: data\\competition\\beauty_train.parquet"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    cfg = Config()\n",
    "    log = Logger()\n",
    "\n",
    "    data_factory = BeautyPairsFactory(cfg, log)\n",
    "    model_creator = CatBoostPRAUCCreator(cfg)\n",
    "\n",
    "    trainer = (\n",
    "        ExperimentBuilder(cfg, log)\n",
    "        .with_data_factory(data_factory)\n",
    "        .with_model_creator(model_creator)\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    # Load data\n",
    "    train_df = data_factory.load_train()\n",
    "    test_df = data_factory.load_test()\n",
    "\n",
    "    # Validate\n",
    "    trainer.fit_validate(train_df)\n",
    "\n",
    "    # Fit full + predict\n",
    "    trainer.fit_full(train_df)\n",
    "    test_pred = trainer.predict_test(test_df)\n",
    "\n",
    "    # Save subm\n",
    "    sub = data_factory.build_submission(test_df, test_pred)\n",
    "    out_path = cfg.DATA_DIR / cfg.submission_name\n",
    "    sub.to_csv(out_path, index=False)\n",
    "    log.info(f\"Saved submission: {out_path} (rows={len(sub)})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27d4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
